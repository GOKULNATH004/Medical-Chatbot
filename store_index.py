from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv
import os

# âœ… Load environment variables from .env
load_dotenv()

# âœ… Step 1: Load PDF data
print("ğŸ“‚ Loading PDFs from 'Data/'...")
extracted_data = load_pdf_file(data='Data/')

# âœ… Check extracted text length
if not extracted_data:
    print("âŒ No text extracted from PDFs! Check your Data/ folder.")
    exit()

print(f"âœ… Extracted data length: {len(extracted_data)} characters")

# âœ… Step 2: Split the text into chunks
print("âœ‚ï¸ Splitting text into chunks...")
text_chunks = text_split(extracted_data)

if not text_chunks:
    print("âŒ No text chunks created! Something went wrong.")
    exit()

print(f"âœ… Number of text chunks created: {len(text_chunks)}")

# âœ… Show preview of first few chunks (optional)
for i, chunk in enumerate(text_chunks[:3], 1):
    print(f"\nChunk {i} (length {len(chunk)} chars):\n{chunk[:300]}...\n")

# âœ… Step 3: Download embeddings
print("â¬‡ï¸ Downloading HuggingFace embeddings...")
embeddings = download_hugging_face_embeddings()

# âœ… Step 4: Create FAISS index from text chunks
print("âš™ï¸ Creating FAISS index...")
docsearch = FAISS.from_texts(text_chunks, embedding=embeddings)

# âœ… Step 5: Save FAISS index locally
print("ğŸ’¾ Saving FAISS index locally...")
docsearch.save_local("faiss_index")

print("âœ… FAISS index saved successfully!")